{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imAdityaSatya/DocuChat/blob/main/DocuChat_(version_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N03f6lPLyX2r"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# !pip install pypdf transformers torch\n",
        "!pip install pypdf transformers torch sentence-transformers faiss-cpu\n",
        "!pip install gradio\n",
        "# !pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import gradio as gr\n",
        "from pypdf import PdfReader\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "# --- Configuration ---\n",
        "RESPONSES_FILE = \"responses.json\"  # Store responses externally\n",
        "MAX_CONTEXT_LENGTH = 400\n",
        "STRIDE = 50\n",
        "CONFIDENCE_THRESHOLD = 0.25\n",
        "MAX_HISTORY_LENGTH = 10  # Store up to 10 recent interactions\n",
        "EMBEDDER_MODEL = \"all-mpnet-base-v2\"\n",
        "QA_MODEL = \"deepset/roberta-base-squad2\"\n",
        "QA_TOKENIZER = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "# --- Load Responses ---\n",
        "def load_responses():\n",
        "    if os.path.exists(RESPONSES_FILE):\n",
        "        with open(RESPONSES_FILE, \"r\") as f:\n",
        "            return json.load(f)\n",
        "    else:\n",
        "        return {\n",
        "            \"greetings_in\": [\"hi\", \"hello\", \"hey\", \"good morning\", \"good evening\", \"greetings\"],\n",
        "            \"greetings_out\": [\"Hello!\", \"Hi there!\", \"Hey! How can I help?\"],\n",
        "\n",
        "            \"intro_in\": [\"who are you\", \"what are you\", \"what is your name\", \"what's your name\", \"whats your name\", \"are you a bot\", \"are you a chatbot\", \"introduce yourself\"],\n",
        "            \"intro_out\": [\"I am DocuChat, a QnA chatbot that answers questions from your PDF.\", \"I'm DocuChat, a chatbot that can answer questions from your PDF\", \"DocuChat here! You can chat with me based on any PDF you upload.\"],\n",
        "\n",
        "            \"ability_in\": [\"what can you do\",  \"how can you help me\", \"how can you help\", \"how can you assist me\"],\n",
        "            \"ability_out\": [\"I can answer questions from your PDF. Ask me anything!\", \"I can answer your queries based on any PDF document uploaded by you\"],\n",
        "\n",
        "            \"creator_in\": [\"who built you\", \"who created you\", \"who made you\", \"who's your creator\", \"who is your creator\", \"who developed you\"],\n",
        "            \"creator_out\": [\"I was created by Aditya Satya.\", \"Aditya Satya has developed me\"],\n",
        "\n",
        "            \"gratitude_in\": [\"good\", \"great\", \"awesome\", \"cool\", \"wow\", \"nice\", \"got it\", \"ok\", \"okay\", \"alright\"],\n",
        "            \"gratitude_out\": [\"Happy to help! Anything else you wanna ask?\", \"Cool! Let me know if you've more questions.\"],\n",
        "\n",
        "            \"gratitude_in_2\": [\"thanks\", \"thank you\", \"thank you so much\", \"thanks a lot\"],\n",
        "            \"gratitude_out_2\": [\"You're welcome! Let me know if you wanna ask anything else\", \"Oh, you're welcome! Is there any other question I can help you with?\"],\n",
        "\n",
        "            \"greet_in\": [\"how are you\", \"how are you doing\", \"how are you feeling\"],\n",
        "            \"greet_out\": [\"I'm doing great, thanks for asking. How may I help you?\", \"Oh, I'm fine. How can I assist you today?\"],\n",
        "\n",
        "            \"sure_in\": [\"are you sure\", \"you sure\", \"really\"],\n",
        "            \"sure_out\": \"Yes, to the best of my knowledge. \\nHowever, I might be wrong sometimes. Please verify if it seems doubtful.\",\n",
        "\n",
        "            \"wrong_in\": [\"incorrect\", \"wrong\", \"you're wrong\", \"you're incorrect\", \"you are wrong\", \"you are wrong\"],\n",
        "            \"wrong_out\": [\"Oops! Well, I could be wrong sometimes. \\nPlease verify if it seems incorrect.\",\n",
        "                          \"I'm sorry if there was a mistake. \\nKindly verify it once from your own end.\"],\n",
        "\n",
        "            \"exit_commands\": [\"exit\", \"stop\", \"quit\", \"bye\", \"goodbye\", \"see ya\", \"terminate\", \"no\"],\n",
        "            \"empty_input_response\": \"Your input is empty. Please ask a question.\",\n",
        "            \"default_response\": \"Sorry, I'm not really sure about that. Please rephrase your question or provide more context.\"\n",
        "        }\n",
        "\n",
        "responses = load_responses()\n",
        "\n",
        "# --- PDF Handling ---\n",
        "def extract_pdf_text(path: str) -> str:\n",
        "    try:\n",
        "        reader = PdfReader(path)\n",
        "        return \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error extracting text from PDF: {e}\")\n",
        "\n",
        "def chunk_text(text: str) -> list:\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i : i + MAX_CONTEXT_LENGTH]) for i in range(0, len(words), MAX_CONTEXT_LENGTH - STRIDE)]\n",
        "\n",
        "# --- Initializing Models ---\n",
        "embedder = SentenceTransformer(EMBEDDER_MODEL)\n",
        "qa_pipeline = pipeline(\"question-answering\", model=QA_MODEL, tokenizer=QA_TOKENIZER)\n",
        "\n",
        "# --- Chatbot Logic ---\n",
        "def get_bot_response(history, pdf_data):\n",
        "\n",
        "    if not history or history[-1][1] is not None:     # Get the last user message from the history\n",
        "        return history, pdf_data      # If the last message already has a bot reply, do nothing\n",
        "\n",
        "    user_input = history[-1][0]   # Extract the user input from the last history entry\n",
        "    cleaned_input = user_input.lower().strip().rstrip(\"?!\")\n",
        "\n",
        "    bot_reply = responses[\"default_response\"]   # Default reply if no specific match or PDF data\n",
        "\n",
        "    if cleaned_input in responses[\"exit_commands\"]:\n",
        "        bot_reply = \"Okay then, Goodbye!\"\n",
        "    elif cleaned_input in responses[\"intro_in\"]:\n",
        "        bot_reply = random.choice(responses[\"intro_out\"])\n",
        "    elif cleaned_input in responses[\"ability_in\"]:\n",
        "        bot_reply = random.choice(responses[\"ability_out\"])\n",
        "    elif cleaned_input in responses[\"creator_in\"]:\n",
        "        bot_reply = random.choice(responses[\"creator_out\"])\n",
        "    elif cleaned_input in responses[\"gratitude_in\"]:\n",
        "        bot_reply = random.choice(responses[\"gratitude_out\"])\n",
        "    elif cleaned_input in responses[\"gratitude_in_2\"]:\n",
        "        bot_reply = random.choice(responses[\"gratitude_out_2\"])\n",
        "    elif cleaned_input in responses[\"greet_in\"]:\n",
        "        bot_reply = random.choice(responses[\"greet_out\"])\n",
        "    elif cleaned_input in responses[\"greetings_in\"]:\n",
        "        bot_reply = random.choice(responses[\"greetings_out\"])\n",
        "    elif cleaned_input in responses[\"sure_in\"]:\n",
        "        bot_reply = responses[\"sure_out\"]\n",
        "    elif cleaned_input in responses[\"wrong_in\"]:\n",
        "        bot_reply = random.choice(responses[\"wrong_out\"])\n",
        "    elif not cleaned_input:\n",
        "        bot_reply = responses[\"empty_input_response\"]\n",
        "    else:\n",
        "        # Handle pdf based questions\n",
        "        try:\n",
        "            if pdf_data is None:\n",
        "                bot_reply = \"Please upload a PDF first. Then we can chat based on that.\"\n",
        "            else:\n",
        "                contexts, faiss_index = pdf_data\n",
        "                q_emb = embedder.encode([user_input], convert_to_numpy=True)\n",
        "                faiss.normalize_L2(q_emb)\n",
        "                D, I = faiss_index.search(q_emb, k=4)\n",
        "                best = {\"score\": 0.0, \"answer\": responses[\"default_response\"]}\n",
        "                for idx in I[0]:\n",
        "                    result = qa_pipeline(question=user_input, context=contexts[idx])\n",
        "                    if result[\"score\"] > best[\"score\"]:\n",
        "                        best = result\n",
        "                # bot_reply = f\"{best['answer']} (Confidence: {best['score']:.2f})\" if best[\"score\"] >= CONFIDENCE_THRESHOLD else best[\"answer\"]\n",
        "                bot_reply = best['answer'] if best['score'] >= CONFIDENCE_THRESHOLD else responses[\"default_response\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            bot_reply = \"I encountered an error. Please try again later.\"\n",
        "\n",
        "    # Update the last entry in history with the bot's reply\n",
        "    history[-1] = (history[-1][0], bot_reply)\n",
        "\n",
        "    # Limit/Trim history length : Ensure history does not exceed MAX_HISTORY_LENGTH\n",
        "    history = history[-MAX_HISTORY_LENGTH:]\n",
        "\n",
        "    return history, pdf_data\n",
        "\n",
        "# --- Gradio Interface ---\n",
        "def upload_pdf(file):\n",
        "    try:\n",
        "        text = extract_pdf_text(file.name)\n",
        "        contexts = chunk_text(text)\n",
        "        faiss_index = build_index(contexts)\n",
        "        return \"PDF uploaded and processed.\", (contexts, faiss_index)\n",
        "    except ValueError as e:\n",
        "        return str(e), None\n",
        "\n",
        "def build_index(ctxs: list):\n",
        "    embeddings = embedder.encode(ctxs, convert_to_numpy=True, show_progress_bar=False)\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    dim = embeddings.shape[1]\n",
        "    idx = faiss.IndexFlatIP(dim)\n",
        "    idx.add(embeddings)\n",
        "    return idx\n",
        "\n",
        "with gr.Blocks(title=\"DocuChat\") as demo:\n",
        "    gr.Markdown(\"# 📄 DocuChat 🤖\\nUpload a PDF and chat with it! 📚\")\n",
        "    with gr.Row():\n",
        "        pdf_uploader = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
        "        # upload_btn = gr.Button(\"Process the file\")\n",
        "\n",
        "    status = gr.Textbox(label=\"Status\", interactive=False, value=\"No file uploaded yet.\")\n",
        "    chatbot = gr.Chatbot(label=\"📄 DocuChat 🤖\")\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        msg = gr.Textbox(label=\"Your Message\", placeholder=\"Ask me anything...\", container=False)\n",
        "        send = gr.Button(\"Send\", scale= 0)  # Scale=0 to ensure an optimal send button width\n",
        "\n",
        "    pdf_data = gr.State(None)  # Store PDF data\n",
        "\n",
        "    pdf_uploader.upload(upload_pdf, inputs=pdf_uploader, outputs=[status, pdf_data])   # Process PDF on upload itself\n",
        "\n",
        "    # msg.submit(get_bot_response, inputs=[msg, chatbot, pdf_data], outputs=[chatbot, pdf_data], queue=True).then(lambda: gr.update(value=''), outputs=msg)\n",
        "    # send.click(get_bot_response, inputs=[msg, chatbot, pdf_data], outputs=[chatbot, pdf_data], queue=True).then(lambda: gr.update(value=''), outputs=msg)\n",
        "\n",
        "    # The event listeners should now call get_bot_response with only chatbot and pdf_data\n",
        "    def user_message_update(user_input, history):\n",
        "        history = history or []\n",
        "        history.append((user_input, None))  # Append user message with a placeholder for bot response\n",
        "        return \"\", history\n",
        "\n",
        "    msg.submit(\n",
        "        user_message_update,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False   # We don't need to queue this immediate update\n",
        "    ).then(\n",
        "        get_bot_response,\n",
        "        inputs=[chatbot, pdf_data],   # get_bot_response now takes the updated history\n",
        "        outputs=[chatbot, pdf_data],\n",
        "        queue=True\n",
        "    )\n",
        "\n",
        "    send.click(\n",
        "        user_message_update,\n",
        "        inputs=[msg, chatbot],\n",
        "        outputs=[msg, chatbot],\n",
        "        queue=False\n",
        "    ).then(\n",
        "        get_bot_response,\n",
        "        inputs=[chatbot, pdf_data],   # get_bot_response now takes the updated history\n",
        "        outputs=[chatbot, pdf_data],\n",
        "        queue=True\n",
        "    )\n",
        "\n",
        "    demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "2odtDecSDSlM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNowYJzvOnKDujG/JbrvCUC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}